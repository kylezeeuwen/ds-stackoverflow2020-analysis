{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bdfcb1-a150-4311-a507-23525f5b8d7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# File Overview\n",
    "\n",
    "This file builds multiple binary classifiers for each country, varying the:\n",
    "\n",
    "* model type (linear regression with rounding, linear svm, and random forest)\n",
    "* the minimum non na cutoff for feature inclusion\n",
    "\n",
    "# Outputs / Assets\n",
    "\n",
    "* saves the result set in a pickle file for subsequent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a5156b-b84f-4b37-81a2-6fcaff091a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_SAMPLE_RATE = 1\n",
    "COUNTRY_MIN_SAMPLE_CUTOFF = 1000\n",
    "DUMMY_NA = True\n",
    "NUMERIC_IMPUTATION_TECHNIQUE=\"Mean\"\n",
    "CUTOFFS = [5000, 2500, 1000, 500, 250, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a3ce56-897d-4e8e-a6af-f355d353af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Developer note: If you modify the local modules, the changes to those modules will not be reflected until you \n",
    "# reload the kernel. This can be fixed by dynamic reloads, but i couldn't get that working\n",
    "# read up on this https://stackoverflow.com/questions/437589/how-do-i-unload-reload-a-python-module\n",
    "\n",
    "pd.options.display.max_rows = 2000\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_colwidth = 255\n",
    "\n",
    "raw_df = pd.read_csv('./assets/survey_results_public.csv')\n",
    "schema = pd.read_csv('./assets/survey_results_schema.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230bf1c-39c0-46fb-a044-3d68fe89cfd5",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "* drop several columns based on bias\n",
    "* did not drop any columns based on lack of values\n",
    "* transform \"near numeric\" columns to numeric\n",
    "* impute numeric columns using mean\n",
    "* impute categorical columns using dummy binary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c69d4-6d60-4e18-ba6f-5bb434c47f28",
   "metadata": {},
   "source": [
    "## Sample Data Set\n",
    "\n",
    "For development purposes, to increase iteration speed, optionally sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb34c1b-faf1-419b-8887-3d09f8c3fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.sample(frac=ROW_SAMPLE_RATE, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9cba5-3e6b-4ddc-a1a4-1eab36ffefd4",
   "metadata": {},
   "source": [
    "Set `Respondent` as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c482d4be-3f52-4c6d-8dab-7023466fed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Respondent' in df.columns:\n",
    "    df.set_index('Respondent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30ff70-666a-47b4-85e9-108434b6b7bf",
   "metadata": {},
   "source": [
    "## Drop Columns\n",
    "\n",
    "* `CompTotal` is a free form numeric that could be hourly, weekly, monthly, or annual compensation, and can be in any currency.\n",
    "* `CompFreq` may be biased towards specific countries. While useful for predicting countries, we want to use developer traits and preferences, not their payment frequency to predict country\n",
    "* The `CurrencyDesc`, `CurrencySymbol`, `Ethnicity` columns all would obviously directly correlate with country of origin. While useful for predicting countries, we want to use developer traits and preferences, not their payment frequency to predict country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e1f307-4b3f-4ee0-a8da-342444ac5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_compensation_column in ['CompTotal', 'CompFreq']:\n",
    "    if raw_compensation_column in df.columns:\n",
    "        df.drop(columns=[raw_compensation_column], inplace=True)\n",
    "\n",
    "for unfair_predictors in ['CurrencyDesc', 'CurrencySymbol', 'Ethnicity']:\n",
    "    if unfair_predictors in df.columns:\n",
    "        df.drop(columns=[unfair_predictors], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10b35b-c7e7-4da3-8b86-1070ce188a48",
   "metadata": {},
   "source": [
    "## Transform \"Near Numeric\" Columns        \n",
    "\n",
    "(based on analysis performed in [01_basics.ipynb](./01_basics.ipynb))\n",
    "\n",
    "The YearsCode, YearsCodePro, and Age1stCode columns are strings but they contain mostly numbers, with a few inequality strings to represent the boundaries.\n",
    "Convert them to numeric so we can treat as a quant metric.\n",
    "\n",
    "While this is losing info - for example \"someone over 50\" is being converted to exactly 51 - this is an acceptable cost when weighed against the benefit of treating these values as numeric vs categorical:\n",
    "\n",
    "* categorical will add X columns to the dataset\n",
    "* if treated as categorical, the models lose the relative proximity of values. In other words 2 is close to 3 but '2' and '3' are unrelated (from a computing perspective)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29aa1dd9-de52-4137-8cb2-d26ed0dac2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from country_classifier.convert_age_series_to_numeric import convert_age_series_to_numeric\n",
    "df['YearsCode'] = df['YearsCode'].map(convert_age_series_to_numeric)\n",
    "df['YearsCodePro'] = df['YearsCodePro'].map(convert_age_series_to_numeric)\n",
    "df['Age1stCode'] = df['Age1stCode'].map(convert_age_series_to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a1b28-dd22-488d-9f3f-d5ff08434627",
   "metadata": {},
   "source": [
    "## Transform \"Choose all that apply\" survey responses\n",
    "\n",
    "* the models we choose cannot work with categorical data, nor can they work with missing values\n",
    "* there are many \"choose all that apply\" questions on the survey. The responses to these are stored in the dataset as a single string, with the individual answers seperated by a ';'\n",
    "* convert these responses into multiple columns, one colums for each individual answer, with a 0 indicating the respondent did not choose that answer, and a 1 indicating they did choose that answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a347cc7c-c5c4-4636-af35-b07793312fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from country_classifier.convert_choose_all_that_apply_responses import convert_choose_all_that_apply_responses\n",
    "df = convert_choose_all_that_apply_responses(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf076e3e-93f2-4170-a742-8455e512d0c3",
   "metadata": {},
   "source": [
    "## Impute numeric columns\n",
    "\n",
    "* we can see ( in [1_basics.ipynb](./1_basics.ipynb)) that all of the numeric columns have at least 50% response rate, so we will not drop any of them\n",
    "* I built models using both mean and mode as the numeric imputation technique and did not see a measurable difference\n",
    "\n",
    "Therefore, for this analysis I will use the mean of the data series to impute all missing numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb603b0-8213-4716-9a3e-4b1f8105160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mean(col): return col.fillna(col.mean())\n",
    "def fill_mode(col): return col.fillna(col.mode()[0])\n",
    "fill = fill_mean if NUMERIC_IMPUTATION_TECHNIQUE == \"Mean\" else fill_mode\n",
    "\n",
    "for column in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    df[column] = fill(df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3ffbe-da2d-4f40-bab7-654e374b84ff",
   "metadata": {},
   "source": [
    "## Impute categorical columns\n",
    "\n",
    "* we can see (in [1_basics.ipynb](./1_basics.ipynb)) that all of the categorical columns have at least 50% response rate, so we will not drop any of them\n",
    "* Our total column count less than 10% of the number of rows, so we can \"afford\" to include a \"no response\" imputed column\n",
    "\n",
    "Therefore, for this analysis I will use standard dummy column imputation to represent the categorical values, and I will include a no response column for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8408006-d54c-4e87-a75f-016c186e84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select_dtypes(include='object').columns:\n",
    "    if column == 'Country':\n",
    "        continue\n",
    "    # for each cat add dummy var, drop original column\n",
    "    df = pd.concat([df.drop(column, axis=1), pd.get_dummies(\n",
    "        df[column], prefix=column, prefix_sep='_', drop_first=True, dummy_na=DUMMY_NA)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8578c5-d167-42ca-b86d-ec6c1703f40f",
   "metadata": {},
   "source": [
    "# Data Modelling\n",
    "\n",
    "* choose which countries to attempt to build a classifier. We want to have enough positives to avoid building a \"naive\" classifier. 1000 seems like a round and reasonable number.\n",
    "* for each country and for each \"min value in column\" cutoff, build 3 models : linear regression, svm, and random forest\n",
    "\n",
    "## Why these models\n",
    "\n",
    "* *linear_regression* : it was the only one included so far in the course material, and it is \"white box\" in that I can query the coefficients and see which inputs it is favoring\n",
    "* *linear support vector machine* : a brief and incomplete survey of recommendations on the Internet suggest that SVMs make good classifiers, but are expensive. So use the linear version\n",
    "* *random forest* : a brief and incomplete survey of recommendations on the Internet suggest random forest provides a classifier that balances between performance and cost\n",
    "\n",
    "## On linear regression model\n",
    "\n",
    "The linear regression model returns a value between 0 and 1, and this analysis calls for a binary 0 or 1. I round the response.\n",
    "This represents a loss of the \"confidence\" of the model, but for this basic analysis, confidence is not used. If there is follow up work, we may consider the confidence, \n",
    "and use to help overcome the imbalanced nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b06a9f4f-6929-41d1-8b8b-b591552a2567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['United States', 'India', 'United Kingdom', 'Germany', 'Canada',\n",
       "       'France', 'Brazil', 'Netherlands', 'Poland', 'Australia', 'Spain',\n",
       "       'Italy', 'Russian Federation'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = df['Country'].value_counts()\n",
    "countries_with_enough_samples = countries[countries > (COUNTRY_MIN_SAMPLE_CUTOFF * ROW_SAMPLE_RATE)].index.values\n",
    "countries_with_enough_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da838e-005d-4aab-90e7-5712b146f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from country_classifier.predict_country import predict_country\n",
    "\n",
    "results = []\n",
    "for country in countries_with_enough_samples:\n",
    "    country_results = predict_country( \\\n",
    "        rowset_label='all_responses', \\\n",
    "        columnset_label='all', \\\n",
    "        imputed_df=df, \\\n",
    "        cutoffs=CUTOFFS, \\\n",
    "        country_to_classify=country, \\\n",
    "        row_sample_rate=ROW_SAMPLE_RATE \\\n",
    "    )\n",
    "    results.extend(country_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115fded-aac3-4c9c-a77e-86a1a11a199e",
   "metadata": {},
   "source": [
    "## Save model results\n",
    "\n",
    "This is done as a developer aid to make it easier to iterate on the evaluation without rerunning the expensive train and test phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "199ec1a2-f4ed-4740-9e3a-18f873606d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(data=results).set_index(['country', 'cutoff'])\n",
    "for to_round in ['TPR', 'TNR', 'PPV', 'NPV', 'ACC', 'duration']:\n",
    "    results_df[to_round] = results_df[to_round].map(lambda x: round(x,3))\n",
    "\n",
    "results_df.drop(columns=['X_columns', 'coefficients'])\n",
    "\n",
    "with open('./pickles/latest_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_df, f)\n",
    "\n",
    "cutoff_string = '-'.join(map(str, CUTOFFS))\n",
    "dataset_name = 'full_country_row_sample_rate_%s_country_min_sample_cutoff_%s_cutoffs_%s_results' % (ROW_SAMPLE_RATE, COUNTRY_MIN_SAMPLE_CUTOFF, cutoff_string)\n",
    "with open('./pickles/%s.pkl' % dataset_name, 'wb') as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e8297-14c0-4bf5-a56a-bfa3d7000c65",
   "metadata": {},
   "source": [
    "# Data Evaluation\n",
    "\n",
    "See [6_visualise_binary_classifier_results.ipynb](./6_visualise_binary_classifier_results.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
