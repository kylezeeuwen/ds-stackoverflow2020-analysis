{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b540d18-dd73-444f-87a1-1421d177f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 2000\n",
    "pd.options.display.max_colwidth = 255\n",
    "\n",
    "df = pd.read_csv('./assets/survey_results_public.csv')\n",
    "schema = pd.read_csv('./assets/survey_results_schema.csv')\n",
    "\n",
    "DUMMY_NA = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8351ec7-b9d1-4e70-9406-68a3e03546b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use respondent as the index\n",
    "if 'Respondent' in df.columns:    \n",
    "    df.set_index('Respondent')\n",
    "\n",
    "# Exclude CompTotal as it is pre converted and normalised to annual USD , which is stored in ConvertedComp\n",
    "if 'CompTotal' in df.columns:\n",
    "    df.drop(columns=['CompTotal'], inplace=True)\n",
    "    \n",
    "# convert these near numerics to a numeric Series\n",
    "from helpers import convert_age_series_to_numeric\n",
    "df['YearsCode'] = df['YearsCode'].map(convert_age_series_to_numeric)\n",
    "df['YearsCodePro'] = df['YearsCodePro'].map(convert_age_series_to_numeric)\n",
    "df['Age1stCode'] = df['Age1stCode'].map(convert_age_series_to_numeric)\n",
    "\n",
    "country_subset = df.query(\"Country in ['United States', 'Canada', 'United Kingdom', 'Australia', 'New Zealand']\")[[\n",
    "    'Country',\n",
    "    'Age',\n",
    "    'Age1stCode',\n",
    "    'ConvertedComp',\n",
    "    'Employment',\n",
    "    'JobSat',\n",
    "    'JobSeek',\n",
    "    'MainBranch',\n",
    "    'NEWEdImpt',\n",
    "    'OpSys',\n",
    "    'OrgSize',\n",
    "    'UndergradMajor',\n",
    "    'WorkWeekHrs',\n",
    "    'YearsCode',\n",
    "    'YearsCodePro'\n",
    "]]\n",
    "\n",
    "categorical_only_columns = country_subset.select_dtypes(include='object').columns\n",
    "for var in categorical_only_columns:\n",
    "    if var == 'Country':\n",
    "        continue\n",
    "    # for each cat add dummy var, drop original column\n",
    "    country_subset = pd.concat([country_subset.drop(var, axis=1), pd.get_dummies(country_subset[var], prefix=var, prefix_sep='_', drop_first=True, dummy_na=DUMMY_NA)], axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568b6253-6bc1-4297-9c78-6984ef519458",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mean = lambda col: col.fillna(col.mean())\n",
    "fill_mode = lambda col: col.fillna(col.mode()[0])\n",
    "\n",
    "country_subset['YearsCodePro'] = fill_mode(country_subset['YearsCodePro'])\n",
    "country_subset['Age'] = fill_mode(country_subset['Age'])\n",
    "country_subset['Age1stCode'] = fill_mode(country_subset['Age1stCode'])\n",
    "country_subset['ConvertedComp'] = fill_mode(country_subset['ConvertedComp'])\n",
    "country_subset['WorkWeekHrs'] = fill_mode(country_subset['WorkWeekHrs'])\n",
    "country_subset['YearsCode'] = fill_mode(country_subset['YearsCode'])\n",
    "country_subset['YearsCodePro'] = fill_mode(country_subset['YearsCodePro'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7fb090-6aaa-4332-95cf-be82e91e43cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting is_usa: r-squared: 0.042, test size 6036, train size 14081, feature size 48.\n",
      "predicting is_south_hemi: r-squared: 0.015, test size 6036, train size 14081, feature size 48.\n",
      "predicting is_south_hemi w/ no uk: r-squared: 0.015, test size 4867, train size 11354, feature size 48.\n"
     ]
    }
   ],
   "source": [
    "is_usa = lambda country: 1 if country == 'United States' else 0\n",
    "is_south_hemi = lambda country: 1 if country in ['Australia', 'New Zealand'] else 0\n",
    "country_subset['is_south_hemi'] = country_subset['Country'].map(is_south_hemi)\n",
    "country_subset['is_usa'] = country_subset['Country'].map(is_usa)\n",
    "\n",
    "# Attempt to predict is_usa\n",
    "X_columns = [ elem for elem in country_subset.columns if elem not in ['Country', 'is_usa', 'is_south_hemi']]\n",
    "y_column = 'is_usa'\n",
    "X = country_subset[X_columns]\n",
    "y = country_subset[y_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42) \n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "y_test_preds = lm_model.predict(X_test) #Predict and score the model \n",
    "print(\"predicting is_usa: r-squared: {}, test size {}, train size {}, feature size {}.\".format(round(r2_score(y_test, y_test_preds),3), len(y_test), len(y_train), X.shape[1]))\n",
    "\n",
    "# Attempt to predict is_south_hemi\n",
    "X_columns = [ elem for elem in country_subset.columns if elem not in ['Country', 'is_usa', 'is_south_hemi']]\n",
    "y_column = 'is_south_hemi'\n",
    "X = country_subset[X_columns]\n",
    "y = country_subset[y_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42) \n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "y_test_preds = lm_model.predict(X_test) #Predict and score the model \n",
    "print(\"predicting is_south_hemi: r-squared: {}, test size {}, train size {}, feature size {}.\".format(round(r2_score(y_test, y_test_preds),3), len(y_test), len(y_train), X.shape[1]))\n",
    "\n",
    "# Attempt to predict is_south_hemi without UK\n",
    "X_columns = [ elem for elem in country_subset.columns if elem not in ['Country', 'is_usa', 'is_south_hemi']]\n",
    "y_column = 'is_south_hemi'\n",
    "no_uk = country_subset[country_subset['Country'] != 'United Kingdom']\n",
    "X = no_uk[X_columns]\n",
    "y = no_uk[y_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42) \n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "y_test_preds = lm_model.predict(X_test) #Predict and score the model \n",
    "print(\"predicting is_south_hemi w/ no uk: r-squared: {}, test size {}, train size {}, feature size {}.\".format(round(r2_score(y_test, y_test_preds),3), len(y_test), len(y_train), X.shape[1]))\n",
    "\n",
    "# dummy_na=False\n",
    "# predicting is_usa: r-squared: 0.041, test size 6036, train size 14081, feature size 48.\n",
    "# predicting is_south_hemi: r-squared: 0.015, test size 6036, train size 14081, feature size 48.\n",
    "# predicting is_south_hemi w/ no uk: r-squared: 0.015, test size 4867, train size 11354, feature size 48.\n",
    "\n",
    "# dummy_na=True\n",
    "# predicting is_usa: r-squared: 0.044, test size 6036, train size 14081, feature size 56.\n",
    "# predicting is_south_hemi: r-squared: 0.016, test size 6036, train size 14081, feature size 56.\n",
    "# predicting is_south_hemi w/ no uk: r-squared: 0.014, test size 4867, train size 11354, feature size 56.\n",
    "\n",
    "#dummy_na=False fill=mode\n",
    "# predicting is_usa: r-squared: 0.042, test size 6036, train size 14081, feature size 48.\n",
    "# predicting is_south_hemi: r-squared: 0.015, test size 6036, train size 14081, feature size 48.\n",
    "# predicting is_south_hemi w/ no uk: r-squared: 0.015, test size 4867, train size 11354, feature size 48.\n",
    "\n",
    "#dummy_na=True fill=mode\n",
    "# predicting is_usa: r-squared: 0.044, test size 6036, train size 14081, feature size 56.\n",
    "# predicting is_south_hemi: r-squared: 0.016, test size 6036, train size 14081, feature size 56.\n",
    "# predicting is_south_hemi w/ no uk: r-squared: 0.014, test size 4867, train size 11354, feature size 56.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2428b046-5a4c-46b1-a5bd-e16068f2e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to filter by employement status\n",
    "\n",
    "employed_country_subset = df \\\n",
    "    .query(\"Country in ['United States', 'Canada', 'United Kingdom', 'Australia', 'New Zealand']\") \\\n",
    "    .query(\"Employment in ['Employed full-time', 'Employed part-time', 'Independent contractor, freelancer, or self-employed']\")[[\n",
    "        'Country',\n",
    "        'Age',\n",
    "        'Age1stCode',\n",
    "        'ConvertedComp',\n",
    "        'JobSat',\n",
    "        'JobSeek',\n",
    "        'MainBranch',\n",
    "        'NEWEdImpt',\n",
    "        'OpSys',\n",
    "        'OrgSize',\n",
    "        'UndergradMajor',\n",
    "        'WorkWeekHrs',\n",
    "        'YearsCode',\n",
    "        'YearsCodePro'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c91ef8a-ecd5-4d1d-bf56-eefacd6b0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_only_columns = employed_country_subset.select_dtypes(include='object').columns\n",
    "for var in categorical_only_columns:\n",
    "    if var == 'Country':\n",
    "        continue\n",
    "    # for each cat add dummy var, drop original column\n",
    "    employed_country_subset = pd.concat([employed_country_subset.drop(var, axis=1), pd.get_dummies(employed_country_subset[var], prefix=var, prefix_sep='_', drop_first=True, dummy_na=DUMMY_NA)], axis=1)    \n",
    "    \n",
    "fill_mean = lambda col: col.fillna(col.mean())\n",
    "fill_mode = lambda col: col.fillna(col.mode()[0])\n",
    "\n",
    "employed_country_subset['YearsCodePro'] = fill_mode(employed_country_subset['YearsCodePro'])\n",
    "employed_country_subset['Age'] = fill_mode(employed_country_subset['Age'])\n",
    "employed_country_subset['Age1stCode'] = fill_mode(employed_country_subset['Age1stCode'])\n",
    "employed_country_subset['ConvertedComp'] = fill_mode(employed_country_subset['ConvertedComp'])\n",
    "employed_country_subset['WorkWeekHrs'] = fill_mode(employed_country_subset['WorkWeekHrs'])\n",
    "employed_country_subset['YearsCode'] = fill_mode(employed_country_subset['YearsCode'])\n",
    "employed_country_subset['YearsCodePro'] = fill_mode(employed_country_subset['YearsCodePro'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "252aab98-acbd-4a29-a647-7b9fb262eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting is_usa: r-squared: 0.048, test size 5220, train size 12177, feature size 42.\n",
      "predicting is_south_hemi: r-squared: 0.013, test size 5220, train size 12177, feature size 42.\n",
      "predicting is_south_hemi w/ no uk: r-squared: 0.016, test size 4185, train size 9763, feature size 42.\n"
     ]
    }
   ],
   "source": [
    "is_usa = lambda country: 1 if country == 'United States' else 0\n",
    "is_south_hemi = lambda country: 1 if country in ['Australia', 'New Zealand'] else 0\n",
    "employed_country_subset['is_south_hemi'] = employed_country_subset['Country'].map(is_south_hemi)\n",
    "employed_country_subset['is_usa'] = employed_country_subset['Country'].map(is_usa)\n",
    "\n",
    "# Attempt to predict is_usa\n",
    "X_columns = [ elem for elem in employed_country_subset.columns if elem not in ['Country', 'is_usa', 'is_south_hemi']]\n",
    "y_column = 'is_usa'\n",
    "X = employed_country_subset[X_columns]\n",
    "y = employed_country_subset[y_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42) \n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "y_test_preds = lm_model.predict(X_test) #Predict and score the model \n",
    "print(\"predicting is_usa: r-squared: {}, test size {}, train size {}, feature size {}.\".format(round(r2_score(y_test, y_test_preds),3), len(y_test), len(y_train), X.shape[1]))\n",
    "\n",
    "# Attempt to predict is_south_hemi\n",
    "X_columns = [ elem for elem in employed_country_subset.columns if elem not in ['Country', 'is_usa', 'is_south_hemi']]\n",
    "y_column = 'is_south_hemi'\n",
    "X = employed_country_subset[X_columns]\n",
    "y = employed_country_subset[y_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42) \n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "y_test_preds = lm_model.predict(X_test) #Predict and score the model \n",
    "print(\"predicting is_south_hemi: r-squared: {}, test size {}, train size {}, feature size {}.\".format(round(r2_score(y_test, y_test_preds),3), len(y_test), len(y_train), X.shape[1]))\n",
    "\n",
    "# Attempt to predict is_south_hemi without UK\n",
    "X_columns = [ elem for elem in employed_country_subset.columns if elem not in ['Country', 'is_usa', 'is_south_hemi']]\n",
    "y_column = 'is_south_hemi'\n",
    "no_uk = employed_country_subset[employed_country_subset['Country'] != 'United Kingdom']\n",
    "X = no_uk[X_columns]\n",
    "y = no_uk[y_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state=42) \n",
    "lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "lm_model.fit(X_train, y_train) #Fit\n",
    "y_test_preds = lm_model.predict(X_test) #Predict and score the model \n",
    "print(\"predicting is_south_hemi w/ no uk: r-squared: {}, test size {}, train size {}, feature size {}.\".format(round(r2_score(y_test, y_test_preds),3), len(y_test), len(y_train), X.shape[1]))\n",
    "\n",
    "#dummy_na=False fill=mean\n",
    "# predicting is_usa: r-squared: 0.047, test size 5220, train size 12177, feature size 42.\n",
    "# predicting is_south_hemi: r-squared: 0.013, test size 5220, train size 12177, feature size 42.\n",
    "# predicting is_south_hemi w/ no uk: r-squared: 0.016, test size 4185, train size 9763, feature size 42.\n",
    "\n",
    "#dummy_na=True fill=mean\n",
    "# predicting is_usa: r-squared: 0.05, test size 5220, train size 12177, feature size 49.\n",
    "# predicting is_south_hemi: r-squared: 0.012, test size 5220, train size 12177, feature size 49.\n",
    "# predicting is_south_hemi w/ no uk: r-squared: 0.018, test size 4185, train size 9763, feature size 49.\n",
    "\n",
    "#dummy_na=False fill=mode\n",
    "# predicting is_usa: r-squared: 0.048, test size 5220, train size 12177, feature size 42.\n",
    "# predicting is_south_hemi: r-squared: 0.013, test size 5220, train size 12177, feature size 42.\n",
    "# predicting is_south_hemi w/ no uk: r-squared: 0.016, test size 4185, train size 9763, feature size 42.\n",
    "\n",
    "#dummy_na=True fill=mode\n",
    "# predicting is_usa: r-squared: 0.05, test size 5220, train size 12177, feature size 49.\n",
    "# predicting is_south_hemi: r-squared: 0.012, test size 5220, train size 12177, feature size 49.\n",
    "# predicting is_south_hemi w/ no uk: r-squared: 0.018, test size 4185, train size 9763, feature size 49.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da838e-005d-4aab-90e7-5712b146f988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
